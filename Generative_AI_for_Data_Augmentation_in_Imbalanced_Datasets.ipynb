{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI for Data Augmentation in Imbalanced Datasets"
      ],
      "metadata": {
        "id": "G4VfiiAuPuvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aims to use generative AI techniques to create synthetic data for addressing the problem of class imbalance in datasets. Imbalanced datasets are common in real-world scenarios and can lead to poor performance in machine learning models. By generating synthetic data points for underrepresented classes, this project seeks to create more balanced datasets, improving the performance of classification models."
      ],
      "metadata": {
        "id": "gE2gPSSJPwzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "9keDEqzRPpXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7tQg8V2vPfG0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Dropout\n",
        "from keras.models import Model\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess data"
      ],
      "metadata": {
        "id": "L_KuYC4fQS3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('Class', axis=1)\n",
        "    y = df['Class']\n",
        "\n",
        "    # Scale the Time and Amount features\n",
        "    scaler = StandardScaler()\n",
        "    X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Extract the minority class from the training set\n",
        "    X_train_minority = X_train[y_train == 1]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, X_train_minority\n",
        "\n",
        "X_train, X_test, y_train, y_test, X_train_minority = load_and_preprocess_data()"
      ],
      "metadata": {
        "id": "91tn6PbePrib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and train VAE"
      ],
      "metadata": {
        "id": "ujFcfESdQV0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "def build_vae(input_dim, intermediate_dim, latent_dim):\n",
        "    # Encoder\n",
        "    inputs = Input(shape=(input_dim,), name='encoder_input')\n",
        "    x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "    # Decoder\n",
        "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "    x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "    outputs = Dense(input_dim, activation='sigmoid')(x)\n",
        "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "    # VAE\n",
        "    outputs = decoder(encoder(inputs)[2])\n",
        "    vae = Model(inputs, outputs, name='vae')\n",
        "\n",
        "    # Loss function\n",
        "    reconstruction_loss = BinaryCrossentropy(inputs, outputs) * input_dim\n",
        "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "    vae.add_loss(vae_loss)\n",
        "\n",
        "    return vae\n",
        "\n",
        "def train_vae(vae, x_train_minority, batch_size, epochs):\n",
        "    # Compile the VAE\n",
        "    vae.compile(optimizer=Adam(learning_rate=0.001), metrics=None)\n",
        "\n",
        "    # Train the VAE on the minority class\n",
        "    vae.fit(x_train_minority, x_train_minority,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_split=0.1)\n",
        "\n",
        "latent_dim = 100\n",
        "vae, encoder, decoder = build_vae(X_train_minority.shape[1], latent_dim)\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "train_vae(vae, X_train_minority, batch_size, epochs)"
      ],
      "metadata": {
        "id": "aKtd_eEUQLnS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the generated data"
      ],
      "metadata": {
        "id": "ZQ7GooDORU_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(vae, n_samples, latent_dim):\n",
        "    z_samples = np.random.normal(0, 1, size=(n_samples, latent_dim))\n",
        "    generated_data = vae.get_layer('decoder')(z_samples)\n",
        "    return K.eval(generated_data)\n",
        "\n",
        "def plot_tsne(real_data, generated_data):\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    combined_data = np.vstack((real_data, generated_data))\n",
        "    tsne_results = tsne.fit_transform(combined_data)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(tsne_results[:len(real_data), 0], tsne_results[:len(real_data), 1], c='b', label='Real Data')\n",
        "    plt.scatter(tsne_results[len(real_data):, 0], tsne_results[len(real_data):, 1], c='r', label='Generated Data')\n",
        "    plt.legend()\n",
        "    plt.title('t-SNE Visualization of Real and Generated Data')\n",
        "    plt.show()\n",
        "    \n",
        "def evaluate_generated_data(x_train_minority, vae, n_samples, latent_dim):\n",
        "    # Generate synthetic data points\n",
        "    generated_data = generate_data(vae, n_samples, latent_dim)\n",
        "\n",
        "    # Visualize the real and generated data using t-SNE\n",
        "    plot_tsne(x_train_minority, generated_data)\n",
        "\n",
        "    return x_train_minority, generated_data\n",
        "n_samples = len(X_train_minority)\n",
        "real_data, generated_data = evaluate_generated_data(X_train_minority, vae, n_samples, latent_dim)"
      ],
      "metadata": {
        "id": "zDVI5DdCQkdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and train classifier with augmented data"
      ],
      "metadata": {
        "id": "TEB7KWMYRlxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(input_dim):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    classifier.add(Dropout(0.2))\n",
        "    classifier.add(Dense(64, activation='relu'))\n",
        "    classifier.add(Dropout(0.2))\n",
        "    classifier.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    classifier.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def augment_training_set(X_train, y_train, generated_data):\n",
        "    X_train_augmented = np.vstack((X_train, generated_data))\n",
        "    y_train_augmented = np.hstack((y_train, np.ones(generated_data.shape[0])))\n",
        "    return X_train_augmented, y_train_augmented\n",
        "\n",
        "def train_classifier(classifier, X_train_augmented, y_train_augmented, batch_size, epochs):\n",
        "    classifier.fit(X_train_augmented, y_train_augmented,\n",
        "                   batch_size=batch_size,\n",
        "                   epochs=epochs,\n",
        "                   verbose=1,\n",
        "                   validation_split=0.1)\n",
        "    \n",
        "classifier = build_classifier(X_train.shape[1])\n",
        "X_train_augmented, y_train_augmented = augment_training_set(X_train, y_train, generated_data)\n",
        "train_classifier(classifier, X_train_augmented, y_train_augmented, batch_size, epochs)"
      ],
      "metadata": {
        "id": "x9r51LTdRrSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate classifier performance with baseline"
      ],
      "metadata": {
        "id": "vRXhVXs0R3z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(classifier, X_test, y_test):\n",
        "    y_pred = classifier.predict(X_test).round()\n",
        "    y_prob = classifier.predict_proba(X_test)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # ROC-AUC score\n",
        "    print(\"ROC-AUC Score:\")\n",
        "    print(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# Step 7: Compare Performance\n",
        "def train_baseline_classifier(X_train, y_train, input_dim, batch_size, epochs):\n",
        "    baseline_classifier = build_classifier(input_dim)\n",
        "    baseline_classifier.fit(X_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            verbose=1,\n",
        "                            validation_split=0.1)\n",
        "    return baseline_classifier\n",
        "\n",
        "def compare_classifiers(baseline_classifier, augmented_classifier, X_test, y_test):\n",
        "    print(\"Baseline Classifier Performance:\")\n",
        "    evaluate_classifier(baseline_classifier, X_test, y_test)\n",
        "\n",
        "    print(\"\\nAugmented Classifier Performance:\")\n",
        "    evaluate_classifier(augmented_classifier, X_test, y_test)\n",
        "    \n",
        "evaluate_classifier(classifier, X_test, y_test)\n",
        "baseline_classifier = train_baseline_classifier(X_train, y_train, X_train.shape[1], batch_size, epochs)\n",
        "compare_classifiers(baseline_classifier, classifier, X_test, y_test)"
      ],
      "metadata": {
        "id": "IOm_5UJbR6SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this Jupyter Notebook, we demonstrated the use of a Variational Autoencoder (VAE) for generating synthetic data to augment a minority class in an imbalanced dataset. We applied this approach to a credit card fraud detection problem, where the number of fraudulent transactions is significantly lower compared to legitimate transactions.\n",
        "\n",
        "We followed these steps:\n",
        "\n",
        "1. Loaded and preprocessed the credit card transaction data, separating the minority class for VAE training.\n",
        "2. Built and trained a VAE to learn the distribution of the minority class (fraudulent transactions).\n",
        "3. Generated synthetic data points using the trained VAE and evaluated their quality using t-SNE visualization.\n",
        "4. Augmented the original training set with the generated data points and trained a classifier on the augmented dataset.\n",
        "5. Evaluated the performance of the classifier trained on the augmented dataset.\n",
        "6. Trained a baseline classifier on the original training set and compared its performance with the classifier trained on the augmented dataset.\n",
        "\n",
        "By comparing the performance of the classifier trained on the augmented dataset with the baseline classifier, we observed the impact of data augmentation using VAE-generated samples on the detection of fraudulent transactions. The results provide insights into the effectiveness of this approach in addressing the class imbalance problem and improving the classifier's performance.\n",
        "\n",
        "Future work could explore alternative data generation techniques, fine-tuning the VAE architecture, and experimenting with different classifiers to further enhance the detection of fraudulent transactions. Additionally, further validation using other imbalanced datasets could be performed to evaluate the generalizability of this approach."
      ],
      "metadata": {
        "id": "ro0XnlrXSSRU"
      }
    }
  ]
}